{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk pandas pymorphy3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt_tab') # пакет для токенизации\n",
    "nltk.download('stopwords') # стоп-слова\n",
    "\n",
    "import pymorphy3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "source": [
    "Мы будем анализировать датасет отзывов на предмет того, является ли отзыв положительным, нейтральным или отрицательным. Датасет лежит [здесь](women_clothing_reviews.csv)\n",
    "\n",
    "### Загрузка датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Пример загрузки CSV файла\n",
    "df = pd.read_csv('women_clothing_reviews.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df['text_column_name']\n",
    "labels = df['label_column_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка данных\n",
    "\n",
    "Для проведения частотного анализа и определения тематики текста рекомендуется выполнить очистку текста от знаков пунктуации, лишних пробельных символов и цифр. Сделать это можно различными способами – с помощью встроенных функций работы со строками, с помощью регулярных выражений, с помощью операций обработки списков или другим способом.\n",
    "\n",
    "Вам нужно\n",
    "- удалить пунктуацию\n",
    "- привести слова в нижний регистр\n",
    "- разбить предложения на токены\n",
    "- лемматизировать текст\n",
    "- удалить стоп-слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "# подсказка для удаления пунктуации\n",
    "import string\n",
    "\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Токенизация\n",
    "\n",
    "Для последующей обработки очищенный текст необходимо разбить на составные части – токены. В анализе текста на естественном языке применяется разбиение на символы, слова и предложения. Процесс разбиения называется токенизация. Для нашей задачи частотного анализа необходимо разбить текст на слова. Для этого можно использовать готовый метод библиотеки NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# пример токенизации\n",
    "from nltk import word_tokenize\n",
    "\n",
    "text = \"Ах, как же нравится ездить в 'Сапсане' по маршруту Москва-Петербург; каждый раз как-то удивляюсь видам, проносящимся из-за окна!\"\n",
    "\n",
    "text_tokens = word_tokenize(text, language=\"russian\")\n",
    "text_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Лемматизация\n",
    "\n",
    "Лемматизация сводит слова к их лемме — это сложный процесс, который учитывает морфологический анализ слов. Лемматизация обрабатывает слова, приводя их к словарной форме. Для русского можно пользоваться лемматизацией из nltk, можно из pymorphy (документация [тут](https://pymorphy2.readthedocs.io/en/stable/), нужно устанавливать и пользоваться pymorphy3, потому что версия 2 не рабоатет на новых версиях питона)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# пример получения разборов для слова\n",
    "\n",
    "morph = pymorphy3.MorphAnalyzer()\n",
    "morph.parse('стали')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Удаление стоп-слов\n",
    "К стоп-словам (или шумовым словам), как правило, относят предлоги, союзы, междометия, частицы и другие части речи, которые часто встречаются в тексте, являются служебными и не несут смысловой нагрузки – являются избыточными.\n",
    "\n",
    "Библиотека NLTK содержит готовые списки стоп-слов для различных языков. Получим список стоп-слов для русского языка:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "russian_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание (2 балла)\n",
    "\n",
    "Напишите функцию, которая предобрабатывает текст\n",
    "\n",
    "Вам нужно\n",
    "- удалить пунктуацию\n",
    "- привести слова в нижний регистр\n",
    "- разбить предложения на токены\n",
    "- лемматизировать текст\n",
    "- удалить стоп-слова\n",
    "\n",
    "Посчитайте, сколько времени это занимает на всем датасете. Подсказка: используйте библиотеку tqdm. Сохраните предобработанный текст в отдельной переменной"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ВАШ КОД ЗДЕСЬ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Разделение выборки\n",
    "\n",
    "Разделите выборку с предобработкой на обучающую (80%) и тестовую (20%) часть. Сделайте то же с выборкой без предобработки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ВАШ КОД ЗДЕСЬ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Кодирование текста (2 балла)\n",
    "\n",
    "Используйте следующие методы кодирования текста:\n",
    "\n",
    "**Bag of Words (BoW):**\n",
    "- Преобразуйте текст в числовой формат, используя метод мешка слов.\n",
    "- Используйте CountVectorizer из библиотеки sklearn.\n",
    "\n",
    "**TF-IDF:**\n",
    "- Преобразуйте текст в числовой формат, используя метод TF-IDF.\n",
    "- Используйте TfidfVectorizer из библиотеки sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "### Пример работы с Bag of words\n",
    "\n",
    "vectorizer_bow = CountVectorizer()\n",
    "X_train_bow = vectorizer_bow.fit_transform(X_train)\n",
    "X_test_bow = vectorizer_bow.transform(X_test)\n",
    "\n",
    "### Пример работы с TF-IDF\n",
    "vectorizer_tfidf = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer_tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer_tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрите на внутренности BoW и TF-IDF "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обучение модели (2 балла)\n",
    "\n",
    "Обучите модель классификации (например, Logistic Regression) на каждом из закодированных представлений текста. Используйте библиотеку sklearn обучения. Обратите внимание, что вам может потребоваться кодировать метки классов в числа (например, negative = -1, positive = 1, neutral = 0)\n",
    "\n",
    "Оцените качество модели на тестовой выборке, используя метрики:\n",
    "\n",
    "- Точность (Accuracy).\n",
    "- F1-мера (F1-score) для каждого класса.\n",
    "\n",
    "\n",
    "Итого у вас должны получиться модели для\n",
    "- Непредобработанного текста с BoW векторайзером\n",
    "- Непредобработанного текста с Tf-IDF векторайзером\n",
    "- Предобработанного текста с BoW векторайзером\n",
    "- Предобработанного текста с Tf-IDF векторайзером\n",
    "\n",
    "Сравните результаты на метриках. **Напишите выводы**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Пример\n",
    "\n",
    "# Обучение модели на Bag of Words\n",
    "model_bow = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\")\n",
    "model_bow.fit(X_train_bow, y_train)\n",
    "\n",
    "# Оценка модели на Bag of Words\n",
    "y_pred_bow = model_bow.predict(X_test_bow)\n",
    "print(\"Bag of Words:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_bow))\n",
    "print(\"F1-score (по классам):\", f1_score(y_test, y_pred_bow, average=None))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_bow))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_bow))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(+2 допбалла к домашке)** Напишите сами пару выдуманных отзывов и получите предсказания на них на лучшей модели. \n",
    "Не забудьте про то, что текст перед подачей нужно предобрабатывать и векторизовать. Предсказания сошлись с ожиданиями?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Получение векторных представлений и обучение модели на них\n",
    "\n",
    "Мы обсуждали, что есть уже готовые модели, которые выдают для слов вектора, опираясь на гипотезу дистрибутивности - для слов с похожими контекстами вектора будут похожими. \n",
    "\n",
    "Давайте скачаем новейшую модель для русского языка, созданную на основе Национального корпуса русского языка (НКРЯ), и загрузим в её в память (поскольку zip-архив с моделью весит почти 500 мегабайт, следующая ячейка выполнится у вас не сразу!). Распаковывать скачанный архив для обычных моделей не нужно, так как его содержимое прочитается при помощи специальной инструкции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in /opt/anaconda3/lib/python3.12/site-packages (3.2)\n",
      "Requirement already satisfied: gensim in /opt/anaconda3/lib/python3.12/site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in /opt/anaconda3/lib/python3.12/site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/anaconda3/lib/python3.12/site-packages (from gensim) (5.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install wget gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "model_url = 'http://vectors.nlpl.eu/repository/20/180.zip'\n",
    "m = wget.download(model_url)\n",
    "model_file = model_url.split('/')[-1]\n",
    "with zipfile.ZipFile(model_file, 'r') as archive:\n",
    "   stream = archive.open('model.bin')\n",
    "   model = gensim.models.KeyedVectors.load_word2vec_format(stream, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание: Получение векторов из модели для текста (3 балла)\n",
    "\n",
    "Для каждого текста нужно получить вектор каждого слова, а затем усреднить их (np.mean), чтобы получить один вектор для всего текста.\n",
    "Ниже дан пример, как нужно получать вектор из модели для одного слова. Обратите внимание, что к слову нужно прибавлять его часть речи, вы можете это сделать например при разборе через pymorphy3, [вот список используемых частей речи](https://universaldependencies.org/u/pos/all.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### вот как получается вектор из модели\n",
    "\n",
    "word = \"лицей_NOUN\"\n",
    "model[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, не все слова могут быть доступны в модели, это нужно обрабатывать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'вайб_NOUN' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m word \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mвайб_NOUN\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m model[word]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/gensim/models/keyedvectors.py:403\u001b[0m, in \u001b[0;36mKeyedVectors.__getitem__\u001b[0;34m(self, key_or_keys)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get vector representation of `key_or_keys`.\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \n\u001b[1;32m    391\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    400\u001b[0m \n\u001b[1;32m    401\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key_or_keys, _KEY_TYPES):\n\u001b[0;32m--> 403\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_vector(key_or_keys)\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vstack([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_vector(key) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m key_or_keys])\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/gensim/models/keyedvectors.py:446\u001b[0m, in \u001b[0;36mKeyedVectors.get_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_vector\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    423\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the key's vector, as a 1D numpy array.\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \n\u001b[1;32m    425\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    444\u001b[0m \n\u001b[1;32m    445\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 446\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_index(key)\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m norm:\n\u001b[1;32m    448\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfill_norms()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/gensim/models/keyedvectors.py:420\u001b[0m, in \u001b[0;36mKeyedVectors.get_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not present\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key 'вайб_NOUN' not present\""
     ]
    }
   ],
   "source": [
    "word = \"вайб_NOUN\"\n",
    "\n",
    "model[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обучение модели (1 балл)\n",
    "\n",
    "На полученных вектроах для текста обучите ту же модель, которую вы обучали для классификации. Посчитайте метрики. Сделайте выводы."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
