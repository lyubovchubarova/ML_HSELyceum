{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk pandas pymorphy3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt_tab') # пакет для токенизации\n",
    "nltk.download('stopwords') # стоп-слова\n",
    "\n",
    "import pymorphy3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "source": [
    "Мы будем анализировать датасет отзывов на предмет того, является ли отзыв положительным, нейтральным или отрицательным. Датасет лежит [здесь](women_clothing_reviews.csv)\n",
    "\n",
    "### Загрузка датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Пример загрузки CSV файла\n",
    "df = pd.read_csv('women_clothing_reviews.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df['text_column_name']\n",
    "labels = df['label_column_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка данных\n",
    "\n",
    "Для проведения частотного анализа и определения тематики текста рекомендуется выполнить очистку текста от знаков пунктуации, лишних пробельных символов и цифр. Сделать это можно различными способами – с помощью встроенных функций работы со строками, с помощью регулярных выражений, с помощью операций обработки списков или другим способом.\n",
    "\n",
    "Вам нужно\n",
    "- удалить пунктуацию\n",
    "- привести слова в нижний регистр\n",
    "- разбить предложения на токены\n",
    "- лемматизировать текст\n",
    "- удалить стоп-слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "# подсказка для удаления пунктуации\n",
    "import string\n",
    "\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Токенизация\n",
    "\n",
    "Для последующей обработки очищенный текст необходимо разбить на составные части – токены. В анализе текста на естественном языке применяется разбиение на символы, слова и предложения. Процесс разбиения называется токенизация. Для нашей задачи частотного анализа необходимо разбить текст на слова. Для этого можно использовать готовый метод библиотеки NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# пример токенизации\n",
    "from nltk import word_tokenize\n",
    "\n",
    "text = \"Ах, как же нравится ездить в 'Сапсане' по маршруту Москва-Петербург; каждый раз как-то удивляюсь видам, проносящимся из-за окна!\"\n",
    "\n",
    "text_tokens = word_tokenize(text, language=\"russian\")\n",
    "text_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Лемматизация\n",
    "\n",
    "Лемматизация сводит слова к их лемме — это сложный процесс, который учитывает морфологический анализ слов. Лемматизация обрабатывает слова, приводя их к словарной форме. Для русского можно пользоваться лемматизацией из nltk, можно из pymorphy (документация [тут](https://pymorphy2.readthedocs.io/en/stable/), нужно устанавливать и пользоваться pymorphy3, потому что версия 2 не рабоатет на новых версиях питона)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# пример получения разборов для слова\n",
    "\n",
    "morph = pymorphy3.MorphAnalyzer()\n",
    "morph.parse('стали')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Удаление стоп-слов\n",
    "К стоп-словам (или шумовым словам), как правило, относят предлоги, союзы, междометия, частицы и другие части речи, которые часто встречаются в тексте, являются служебными и не несут смысловой нагрузки – являются избыточными.\n",
    "\n",
    "Библиотека NLTK содержит готовые списки стоп-слов для различных языков. Получим список стоп-слов для русского языка:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "russian_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание \n",
    "\n",
    "Напишите функцию, которая предобрабатывает текст\n",
    "\n",
    "Вам нужно\n",
    "- удалить пунктуацию\n",
    "- привести слова в нижний регистр\n",
    "- разбить предложения на токены\n",
    "- лемматизировать текст\n",
    "- удалить стоп-слова\n",
    "\n",
    "Посчитайте, сколько времени это занимает на всем датасете. Подсказка: используйте библиотеку tqdm. Сохраните предобработанный текст в отдельной переменной"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ВАШ КОД ЗДЕСЬ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Разделение выборки\n",
    "\n",
    "Разделите выборку с предобработкой на обучающую (80%) и тестовую (20%) часть. Сделайте то же с выборкой без предобработки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ВАШ КОД ЗДЕСЬ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Кодирование текста\n",
    "\n",
    "Используйте следующие методы кодирования текста:\n",
    "\n",
    "**Bag of Words (BoW):**\n",
    "- Преобразуйте текст в числовой формат, используя метод мешка слов.\n",
    "- Используйте CountVectorizer из библиотеки sklearn.\n",
    "\n",
    "**TF-IDF:**\n",
    "- Преобразуйте текст в числовой формат, используя метод TF-IDF.\n",
    "- Используйте TfidfVectorizer из библиотеки sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "### Пример работы с Bag of words\n",
    "\n",
    "vectorizer_bow = CountVectorizer()\n",
    "X_train_bow = vectorizer_bow.fit_transform(X_train)\n",
    "X_test_bow = vectorizer_bow.transform(X_test)\n",
    "\n",
    "### Пример работы с TF-IDF\n",
    "vectorizer_tfidf = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer_tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer_tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрите на внутренности BoW и TF-IDF "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обучение модели\n",
    "\n",
    "Обучите модель классификации (например, Logistic Regression) на каждом из закодированных представлений текста. Используйте библиотеку sklearn обучения.\n",
    "\n",
    "Оцените качество модели на тестовой выборке, используя метрики:\n",
    "\n",
    "- Точность (Accuracy).\n",
    "- F1-мера (F1-score) для каждого класса.\n",
    "\n",
    "\n",
    "Итого у вас должны получиться модели для\n",
    "- Непредобработанного текста с BoW векторайзером\n",
    "- Непредобработанного текста с Tf-IDF векторайзером\n",
    "- Предобработанного текста с BoW векторайзером\n",
    "- Предобработанного текста с Tf-IDF векторайзером\n",
    "\n",
    "Сравните результаты на метриках."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Пример\n",
    "\n",
    "# Обучение модели на Bag of Words\n",
    "model_bow = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\")\n",
    "model_bow.fit(X_train_bow, y_train)\n",
    "\n",
    "# Оценка модели на Bag of Words\n",
    "y_pred_bow = model_bow.predict(X_test_bow)\n",
    "print(\"Bag of Words:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_bow))\n",
    "print(\"F1-score (по классам):\", f1_score(y_test, y_pred_bow, average=None))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_bow))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_bow))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
